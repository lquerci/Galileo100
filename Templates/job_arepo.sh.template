#!/bin/bash
#SBATCH --job-name=run_${run_name}
#SBATCH --nodes=${nodes}
#SBATCH --ntasks-per-node=${ntasks-per-node}
#SBATCH --cpus-per-task=${cpus-per-task}
#SBATCH --time=${time}
#SBATCH --error=%x.%j.err
#SBATCH --output=%x.%j.out
#SBATCH --partition=g100_usr_prod
#SBATCH --account=IscrC_UFD-SHF
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=lapo.querci1@unifi.it

# total MPI processes
N_processes=${n_proc}

# where to find arepo
export AREPO="$SCRATCH/LatinHypercubeSampling/mdm_100/Arepo1e4FOF"

# file names 
ArepoParam_File="ArepoParam_FOF.txt"


module purge
module load gcc/10.2.0 fftw/3.3.10--openmpi--4.1.1--gcc--10.2.0 gsl/2.7--gcc--10.2.0
module load hdf5/1.10.7--gcc--10.2.0-threadsafe python/3.11.7--gcc--10.2.0

echo "module loaded"

# check if the IC are present
if [ "$(find -maxdepth 1 -name "*.g2" -print -quit)" ]; then
    echo "IC exist."
else 
    echo "IC does not exist."
    echo "Compute it with job_dice.sh before running this job"

    exit 2
fi

# set to 1 the OpenMP threads 
export OMP_NUM_THREADS=1

# Running the sim
echo "Running the simulation"

mpirun -np ${N_processes} "$AREPO" "${ArepoParam_File}" > arepo_output.txt

# store the exit code from the previous command
exit_code=$?

echo "Running the simulation ended, read output in arepo_output.txt"

echo "exit code = ${exit_code}"

# exit with the store exit code
exit ${exit_code}
